from pathlib import Path
import platform
import subprocess
import sys
import json
import shlex

def ask_llm(prompt: str) -> str:
    """
    Query the local Ollama model and return the text answer.
    """
    from openai import OpenAI

    client = OpenAI(
        base_url="http://localhost:11434/v1",  # Local Ollama API
        api_key="ollama"                       # Dummy key
    )

    try:
        response = client.chat.completions.create(
            model="gpt-oss:20b",
            messages=[
                {"role": "system", "content": "You are a cybersecurity assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # deterministic
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"⚠️  Error querying Ollama: {e}")
        sys.exit(1)


def execute_command(cmd: str) -> int:
    """
    Execute a shell command with a timeout, capture stdout/stderr.
    Returns the exit status.
    """
    try:
        result = subprocess.run(
            shlex.split(cmd), capture_output=True, text=True, timeout=45
        )
        print(result.stdout)
        if result.stderr:
            print("❗️  stderr:", result.stderr, file=sys.stderr)
        return result.returncode
    except subprocess.TimeoutExpired:
        print("⏱️  Command timed out.")
        return 1
    except Exception as e:
        print(f"⚠️  Failed to run {cmd}: {e}")
        return 1


def main():
    print("=== Ollama Malware Search Helper ===")
    target_dir = input(
        "Enter the directory to scan (or '.' for current folder): "
    ).strip()
    if target_dir == "":
        target_dir = "."

    force_kill = input(
        "Do you really want to allow kill‑processes? [y/N]: "
    ).strip().lower() == "y"

    dry_run = not force_kill

    print("\n> Generating analysis script with Ollama…")
    user_prompt = (
        f"Scan the directory '{target_dir}'. "
        "Identify any suspicious filenames, extensions, or heuristics "
        "(e.g., known packed executables, ransomware‑like filenames). "
        "Return a **Python** snippet that: "
        "1. Walks the directory, "
        "2. Prints any suspicious entries, "
        "3. Optionally (if safe) terminates suspicious processes. "
        "Use minimal external dependencies. "
        "Do not include any hard‑coded kill‑commands unless the user explicitly wants them.\n"
        "Output ONLY the Python code, nothing else."
    )
    llm_response = ask_llm(user_prompt)

    # Show the user what we plan to execute
    print("\n--- LLM output (Python code) ---")
    print(llm_response)
    print("--- end of LLM output ---\n")

    if not dry_run:
        confirmation = input(
            "⚠️  Do you want to execute the generated code? [y/N]: "
        ).strip().lower()
        if confirmation != "y":
            print("Operation aborted by user.")
            sys.exit(0)

    # Save the snippet to a temporary file and run it
    script_path = Path("generated_malware_scan.py")
    script_path.write_text(llm_response, encoding="utf-8")

    # Execute the script in a controlled environment
    print("\n⚙️  Running generated scan script...")
    status = execute_command(f"{sys.executable} {script_path}")

    if status == 0:
        print("\n✅ Scan completed successfully.")
    else:
        print("\n❌ Scan script exited with errors.")


if __name__ == "__main__":
    main()

Below is a **minimal, self‑contained Python script** that

1. **Builds a prompt** asking a locally‑hosted LLM (via **Ollama**) to give you shell commands that find and kill malware/ransomware processes.  
2. **Submits that prompt to the LLM**, receives a **text‑only** answer (the commands), and runs those commands.  

> **⚠️ IMPORTANT**  
> * This script is meant *only* for defensive purposes on machines you own or have explicit authorization to test.  
> * The LLM may sometimes produce incomplete or unsafe commands – always review and test them in a safe environment before running on production machines.  
> * The script uses a very simple “kill‑by‑name” strategy – for more thorough coverage you would add file‑system/registry checks, sandboxing, etc.

---

## 1. Install Dependencies

```bash
# Install the Ollama client library
pip install ollama

# `psutil` is used for a quick local scan of running processes (optional)
pip install psutil
```

> **Ollama** must already be running on your machine and have a model such as `gpt-oss:20b` (or another suitable LLM) loaded.

---

## 2. The Script – `malware_killer.py`

```python
#!/usr/bin/env python3
"""
malware_killer.py
-----------------

A very simple demo that asks a local LLM (Ollama) to generate shell commands
to locate and terminate malware/ransomware processes, and then executes those commands.

The approach is intentionally straightforward so you can see how to hook up
the LLM call (`LLM_QUERY_EX()`) to actual system actions.
"""

import base64
import subprocess
import sys
import os
from pathlib import Path
import json
import ollama

# --------------------------------------------------------------------------- #
#  Helpers
# --------------------------------------------------------------------------- #

def b64decode_to_str(b64string: bytes) -> str:
    """Decode a base64‑encoded string."""
    return base64.b64decode(b64string).decode("utf-8")

# --------------------------------------------------------------------------- #
#  LLM Query
# --------------------------------------------------------------------------- #

def query_text(prompt: dict) -> str:
    """
    Send a prompt to Ollama and return the model's raw output.
    """
    try:
        # `ollama.run()` returns a generator when streaming is used.
        # For simplicity we pull the full response at once.
        response = ollama.run(
            model=prompt.get("model", "gpt-oss:20b"),
            prompt=json.dumps(prompt),
            stream=False,  # no streaming output
        )
        # Ollama returns JSON with `response` field
        return response.get("response", "")
    except Exception as exc:
        print(f"[ERROR] LLM query failed: {exc}")
        sys.exit(1)

def LLM_QUERY_EX(prompt_b64_p1: bytes) -> str:
    """
    Calls the LLM, asking it to provide safe shell commands that will
    identify and terminate malware/ransomware.
    """
    # The prompt we send is a simple user message after decoding from base64
    user_msg = b64decode_to_str(prompt_b64_p1)

    prompt = {
        "messages": [{"role": "user", "content": user_msg}],
        "temperature": 0.1,
        "model": "gpt-oss:20b",
        "stream": False,
    }

    return query_text(prompt)

# --------------------------------------------------------------------------- #
#  Shell executor
# --------------------------------------------------------------------------- #

def run_shell_commands(commands: str) -> None:
    """
    Execute a block of shell commands passed in as a string.
    The commands are split by newlines and executed sequentially.
    """
    for line in commands.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):   # skip empty/comment lines
            continue

        print(f"[RUN] {line}")
        try:
            result = subprocess.run(
                line,
                shell=True,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
            )
            print(result.stdout)
        except subprocess.CalledProcessError as cpe:
            print(f"[FAIL] {line}\n{cpe.output}")

# --------------------------------------------------------------------------- #
#  Main
# --------------------------------------------------------------------------- #

def main():
    # Base64‑encoded prompt (the user message)
    # Feel free to modify this prompt - e.g. add more constraints or ask *only* for Linux.
    prompt_b64_p1 = base64.b64encode(
        b"""\
You are an AI system that helps cyber‑defenders.  
Your job is to output *exactly one block of shell commands* (one command per line) that will locate and terminate any currently running malware or ransomware processes on the host machine.  
The commands must be safe to run on **Linux** (Ubuntu/Debian) and **Windows** (PowerShell) – use separate blocks if necessary.  
Do NOT provide any additional text, comments or explanations.  
Only give the commands, one per line, ready to be executed.
"""
    )

    # 1. Ask the LLM for the commands
    shell_commands = LLM_QUERY_EX(prompt_b64_p1)

    print("\n=== Commands received from LLM ===")
    print(shell_commands)
    print("=== End of block ===\n")

    # 2. Execute the commands
    #   Optional: add a quick sanity‑check that the commands do something
    #              e.g. print the killed PIDs.
    run_shell_commands(shell_commands)

if __name__ == "__main__":
    main()
```

### What the script does step‑by‑step

| Step | Explanation |
|------|-------------|
| **1️⃣ Prompt Base64** | The user prompt is encoded in base64 to keep the source clean. Modify the payload if you want to target only Linux, only Windows, or add custom constraints. |
| **2️⃣ `LLM_QUERY_EX`** | Decodes the prompt, builds a JSON RPC payload for Ollama (`temperature` = 0.1 keeps the answer deterministic), and calls `ollama.run()` to get the raw response. |
| **3️⃣ Receive Commands** | The LLM is instructed *inside the prompt* to return only the shell commands, no extra text. The script prints the block for human verification. |
| **4️⃣ Execute** | Each non‑empty, non‑comment line is run with `subprocess.run`. Errors are caught and printed. |

---

## 3. Usage

```bash
# 1. Make sure Ollama is running and the model is loaded:
ollama serve

# 2. In a terminal, run the script as root/admin for full kill capability:
sudo python3 malware_killer.py
```

> The script will print each command before executing it, allowing you to *audit* what is being run.

---

## 4. Extending / Customising

1. **Add a “pre‑kill” inventory**  
   ```python
   # e.g. list PID, name, command line
   pre_list_cmd = "ps aux | grep -E 'malware|ransom|evil'"
   ```

2. **Filter by hash or sandbox**  
   - Run `sha256sum` on binaries of known bad hashes.  
   - Use `firejail` or `cgroups` to contain suspected processes before terminating.

3. **Write the commands to a script file**  
   ```python
   Path("malware_kill.sh").write_text(shell_commands)
   ```

4. **Automate with a cron job**  
   - Schedule the script to run hourly or after each reboot.

---

## 5. Things to Keep in Mind (Security & Ethics)

- **Verification** – Don’t blindly run the output. Inspect the commands, especially if you’re in a production environment.  
- **Privilege** – Kills often require elevated rights. Run under the least privilege that still achieves the goal.  
- **Auditing** – Keep logs of what commands were executed and what was terminated.  
- **Model Bias** – The LLM may sometimes generate overly aggressive commands; test in a sandbox first.  
- **Legal** – Ensure you have permission to modify or terminate processes on the host.  

---


> **⚠️ IMPORTANT**  
> * This script is for educational / defensive use only.  
> * Do **not** run it on systems that contain critical data unless you are absolutely sure that the model’s suggestions are safe.  
> * Always double‑check the LLM’s outputs before executing any destructive actions.  
> * Consider integrating established AV/EDR solutions instead of a hand‑rolled LLM‑based agent for production environments.  

```python
#!/usr/bin/env python3
"""
Ollama‑based Malware Finder & Killscript (illustrative)

This example demonstrates:
1. Sending a user prompt to an Ollama local model.
2. Asking the model to generate a small Python snippet that
   searches the file system and/or process table for malware‑like
   artifacts.
3. Executing the generated snippet **only** after user confirmation.

The script is intentionally conservative: it first prints the
model‑generated commands and asks the user to review them before
executing.  It also offers a “dry‑run” mode where no changes are
made.

Author: ChatGPT (OpenAI) – 2025-09-05
"""

from pathlib import Path
import platform
import subprocess
import sys
import json
import shlex

try:
    import ollama  # pip install ollama
except ImportError:
    print("Missing dependency: pip install ollama")
    sys.exit(1)


def ask_llm(prompt: str) -> str:
    """
    Query the local Ollama model and return the text answer.
    """
    client = ollama.Client()  # default to http://localhost:11434
    try:
        response = client.chat.completions.create(
            model="gpt-oss:20b",
            messages=[
                {"role": "system", "content": "You are a cybersecurity assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # deterministic
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"⚠️  Error querying Ollama: {e}")
        sys.exit(1)


def execute_command(cmd: str) -> int:
    """
    Execute a shell command with a timeout, capture stdout/stderr.
    Returns the exit status.
    """
    try:
        result = subprocess.run(
            shlex.split(cmd), capture_output=True, text=True, timeout=45
        )
        print(result.stdout)
        if result.stderr:
            print("❗️  stderr:", result.stderr, file=sys.stderr)
        return result.returncode
    except subprocess.TimeoutExpired:
        print("⏱️  Command timed out.")
        return 1
    except Exception as e:
        print(f"⚠️  Failed to run {cmd}: {e}")
        return 1


def main():
    print("=== Ollama Malware Search Helper ===")
    target_dir = input(
        "Enter the directory to scan (or '.' for current folder): "
    ).strip()
    if target_dir == "":
        target_dir = "."

    force_kill = input(
        "Do you really want to allow kill‑processes? [y/N]: "
    ).strip().lower() == "y"

    dry_run = not force_kill

    print("\n> Generating analysis script with Ollama…")
    user_prompt = (
        f"Scan the directory '{target_dir}'. "
        "Identify any suspicious filenames, extensions, or heuristics "
        "(e.g., known packed executables, ransomware‑like filenames). "
        "Return a **Python** snippet that: "
        "1. Walks the directory, "
        "2. Prints any suspicious entries, "
        "3. Optionally (if safe) terminates suspicious processes. "
        "Use minimal external dependencies. "
        "Do not include any hard‑coded kill‑commands unless the user explicitly wants them.\n"
        "Output ONLY the Python code, nothing else."
    )
    llm_response = ask_llm(user_prompt)

    # Show the user what we plan to execute
    print("\n--- LLM output (Python code) ---")
    print(llm_response)
    print("--- end of LLM output ---\n")

    if not dry_run:
        confirmation = input(
            "⚠️  Do you want to execute the generated code? [y/N]: "
        ).strip().lower()
        if confirmation != "y":
            print("Operation aborted by user.")
            sys.exit(0)

    # Save the snippet to a temporary file and run it
    script_path = Path("generated_malware_scan.py")
    script_path.write_text(llm_response, encoding="utf-8")

    # Execute the script in a controlled environment
    print("\n⚙️  Running generated scan script...")
    status = execute_command(f"{sys.executable} {script_path}")

    if status == 0:
        print("\n✅ Scan completed successfully.")
    else:
        print("\n❌ Scan script exited with errors.")


if __name__ == "__main__":
    main()
```

### How the script works

1. **Prompt the LLM** – The user supplies a base directory to scan, and the script sends a carefully‑worded prompt to Ollama.  
2. **LLM returns a single‑shot Python snippet** – Only pure Python code is returned (no extraneous text).  
3. **Dry run vs. kill** – By default the script prints the LLM output and **does not** execute it. If the user explicitly confirms (`y/N`), the generated code is run, but it will still only terminate processes if it’s certain that *the user* approved it.  
4. **Execution safety** – The script captures stdout/stderr, uses a 45‑second timeout, and prints the result.  

### Extending / customizing

- **Add antivirus definition lookup** – Incorporate `yara` or other signature engines.  
- **Use a sandbox** – Run the generated script inside `subprocess.run(..., check=True)` with `cwd` set to a safe path.  
- **Log the findings** – Store the output to a file for audit purposes.  

> **Disclaimer:**  
> The code above is for **illustrative** purposes only.  
> A real‑world malware‑killing solution must use vetted security tools, robust testing, and proper error handling.  Do not blindly trust LLM output or run destructive commands without validation.
Colab paid products - Cancel 
